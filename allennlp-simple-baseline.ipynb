{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab6cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25594f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          EMPTY_HOSTNAME\n",
       "1          EMPTY_HOSTNAME\n",
       "2          EMPTY_HOSTNAME\n",
       "3          EMPTY_HOSTNAME\n",
       "4          EMPTY_HOSTNAME\n",
       "              ...        \n",
       "2829    sites.ehe.osu.edu\n",
       "2830     en.wikibooks.org\n",
       "2831     en.wikibooks.org\n",
       "2832     en.wikibooks.org\n",
       "2833     en.wikibooks.org\n",
       "Name: hostname, Length: 2834, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pandas.read_csv(\"data/train.csv\")\n",
    "train_df[\"hostname\"] = train_df \\\n",
    "    .url_legal \\\n",
    "    .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else None) \\\n",
    "    .fillna(\"EMPTY_HOSTNAME\")\n",
    "\n",
    "train_df[\"hostname\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65db1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !poetry run allennlp train training_configs/baseline.jsonnet --serialization-dir serialization/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af1e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import allennlp.commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596f5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, MutableMapping, Optional\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from allennlp.data import DatasetReader\n",
    "from allennlp.data import Tokenizer\n",
    "from allennlp.data.fields.field import Field\n",
    "from allennlp.data.fields import ArrayField, TextField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.token_indexers.token_indexer import TokenIndexer\n",
    "import pandas\n",
    "import numpy\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"commonlit_reader\")\n",
    "class CommonlitDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers: Dict[str, TokenIndexer] = {\n",
    "            \"tokens\": SingleIdTokenIndexer(),\n",
    "        }\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        instances = []\n",
    "\n",
    "        dataframe = pandas.read_csv(file_path)\n",
    "        dataframe[\"hostname\"] = dataframe \\\n",
    "            .url_legal \\\n",
    "            .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n",
    "\n",
    "        for _, row in dataframe.iterrows():\n",
    "            excerpt = row.excerpt\n",
    "            target = row.target if hasattr(row, \"target\") else None\n",
    "            instances.append(self.text_to_instance(excerpt, target))\n",
    "\n",
    "        return instances\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, excerpt: str, target: Optional[float] = None) -> Instance:\n",
    "        tokens = self.tokenizer.tokenize(excerpt)\n",
    "        fields: MutableMapping[str, Field[Any]] = {\n",
    "            \"excerpt\": TextField(tokens),\n",
    "        }\n",
    "        if target is not None:\n",
    "            fields[\"target\"] = ArrayField(numpy.asarray(target, dtype=numpy.float32))\n",
    "        return Instance(fields=fields)\n",
    "\n",
    "    def apply_token_indexers(self, instance: Instance) -> None:\n",
    "        assert isinstance(instance.fields[\"excerpt\"], TextField)\n",
    "        instance.fields[\"excerpt\"].token_indexers = self.token_indexers\n",
    "\n",
    "        \n",
    "from typing import Dict, Optional\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import TextFieldEmbedder\n",
    "from allennlp.modules import Seq2VecEncoder\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.data.fields.text_field import TextFieldTensors\n",
    "from overrides.overrides import overrides\n",
    "from torch import FloatTensor\n",
    "from torch.functional import Tensor\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.nn import Linear\n",
    "\n",
    "\n",
    "@Model.register(\"baseline\")\n",
    "class BaselineRegressor(Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Vocabulary,\n",
    "        excerpt_embedder: TextFieldEmbedder,\n",
    "        excerpt_encoder: Seq2VecEncoder,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.excerpt_embedder = excerpt_embedder\n",
    "        self.excerpt_encoder = excerpt_encoder\n",
    "\n",
    "        self.dense = Linear(\n",
    "            in_features=self.excerpt_encoder.get_output_dim(),\n",
    "            out_features=1,\n",
    "        )\n",
    "\n",
    "    @overrides\n",
    "    def forward(\n",
    "        self,\n",
    "        excerpt: TextFieldTensors,\n",
    "        target: Optional[FloatTensor] = None,\n",
    "    ) -> Dict[str, Tensor]:\n",
    "\n",
    "        mask = get_text_field_mask(excerpt)\n",
    "        excerpt_emb = self.excerpt_embedder(excerpt)\n",
    "        logit = self.dense(self.excerpt_encoder(excerpt_emb, mask=mask))\n",
    "\n",
    "        output_dict = {\"logit\": logit}\n",
    "        if target is not None:\n",
    "            output_dict[\"loss\"] = mse_loss(logit.view(-1), target)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {}\n",
    "\n",
    "    \n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "\n",
    "@Predictor.register(\"regressor_predictor\")\n",
    "class RegressorPredictor(Predictor):\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        return self._dataset_reader.text_to_instance(**json_dict)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff25a0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 12:13:09,986 - INFO - allennlp.common.params - random_seed = 13370\n",
      "2021-07-26 12:13:09,987 - INFO - allennlp.common.params - numpy_seed = 1337\n",
      "2021-07-26 12:13:09,987 - INFO - allennlp.common.params - pytorch_seed = 133\n",
      "2021-07-26 12:13:09,990 - INFO - allennlp.common.checks - Pytorch version: 1.9.0+cu102\n",
      "2021-07-26 12:13:09,991 - INFO - allennlp.common.params - type = default\n",
      "2021-07-26 12:13:09,993 - INFO - allennlp.common.params - dataset_reader.type = commonlit_reader\n",
      "2021-07-26 12:13:09,994 - INFO - allennlp.common.params - dataset_reader.tokenizer = whitespace\n",
      "2021-07-26 12:13:09,995 - INFO - allennlp.common.params - type = whitespace\n",
      "2021-07-26 12:13:09,996 - INFO - allennlp.common.params - train_data_path = data/train.csv\n",
      "2021-07-26 12:13:09,999 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f7218d509a0>\n",
      "2021-07-26 12:13:10,002 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
      "2021-07-26 12:13:10,003 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
      "2021-07-26 12:13:10,004 - INFO - allennlp.common.params - validation_data_path = data/test.csv\n",
      "2021-07-26 12:13:10,005 - INFO - allennlp.common.params - validation_data_loader = None\n",
      "2021-07-26 12:13:10,006 - INFO - allennlp.common.params - test_data_path = None\n",
      "2021-07-26 12:13:10,007 - INFO - allennlp.common.params - evaluate_on_test = False\n",
      "2021-07-26 12:13:10,008 - INFO - allennlp.common.params - batch_weight_key = \n",
      "2021-07-26 12:13:10,009 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
      "2021-07-26 12:13:10,011 - INFO - allennlp.common.params - data_loader.batch_size = 16\n",
      "2021-07-26 12:13:10,012 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
      "2021-07-26 12:13:10,014 - INFO - allennlp.common.params - data_loader.shuffle = True\n",
      "2021-07-26 12:13:10,015 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
      "2021-07-26 12:13:10,034 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
      "2021-07-26 12:13:10,035 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
      "2021-07-26 12:13:10,037 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
      "2021-07-26 12:13:10,038 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
      "2021-07-26 12:13:10,039 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
      "2021-07-26 12:13:10,040 - INFO - allennlp.common.params - data_loader.quiet = False\n",
      "2021-07-26 12:13:10,041 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f721fb97430>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c824c937a24c189490afc4c89ac245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 12:13:11,388 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
      "2021-07-26 12:13:11,389 - INFO - allennlp.common.params - data_loader.batch_size = 16\n",
      "2021-07-26 12:13:11,390 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
      "2021-07-26 12:13:11,391 - INFO - allennlp.common.params - data_loader.shuffle = True\n",
      "2021-07-26 12:13:11,392 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
      "2021-07-26 12:13:11,393 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
      "2021-07-26 12:13:11,394 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
      "2021-07-26 12:13:11,396 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
      "2021-07-26 12:13:11,396 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
      "2021-07-26 12:13:11,398 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
      "2021-07-26 12:13:11,398 - INFO - allennlp.common.params - data_loader.quiet = False\n",
      "2021-07-26 12:13:11,400 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f721fb97430>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ed1771a296404190dbbff5e80a5709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 12:13:11,442 - INFO - allennlp.common.params - type = from_instances\n",
      "2021-07-26 12:13:11,445 - INFO - allennlp.common.params - min_count = None\n",
      "2021-07-26 12:13:11,446 - INFO - allennlp.common.params - max_vocab_size = None\n",
      "2021-07-26 12:13:11,448 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
      "2021-07-26 12:13:11,449 - INFO - allennlp.common.params - pretrained_files = None\n",
      "2021-07-26 12:13:11,450 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
      "2021-07-26 12:13:11,451 - INFO - allennlp.common.params - tokens_to_add = None\n",
      "2021-07-26 12:13:11,451 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
      "2021-07-26 12:13:11,452 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
      "2021-07-26 12:13:11,453 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
      "2021-07-26 12:13:11,453 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293e04c48d3c4e818a0281f97138e273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 12:13:11,806 - INFO - allennlp.common.params - model.type = baseline\n",
      "2021-07-26 12:13:11,807 - INFO - allennlp.common.params - model.excerpt_embedder.type = basic\n",
      "2021-07-26 12:13:11,808 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.type = embedding\n",
      "2021-07-26 12:13:11,809 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.embedding_dim = 50\n",
      "2021-07-26 12:13:11,810 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.num_embeddings = None\n",
      "2021-07-26 12:13:11,811 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.projection_dim = None\n",
      "2021-07-26 12:13:11,811 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.weight = None\n",
      "2021-07-26 12:13:11,813 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.padding_index = None\n",
      "2021-07-26 12:13:11,814 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.trainable = True\n",
      "2021-07-26 12:13:11,815 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.max_norm = None\n",
      "2021-07-26 12:13:11,816 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "2021-07-26 12:13:11,817 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "2021-07-26 12:13:11,818 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.sparse = False\n",
      "2021-07-26 12:13:11,819 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
      "2021-07-26 12:13:11,819 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.pretrained_file = None\n",
      "2021-07-26 12:13:11,833 - INFO - allennlp.common.params - model.excerpt_encoder.type = lstm\n",
      "2021-07-26 12:13:11,834 - INFO - allennlp.common.params - model.excerpt_encoder.input_size = 50\n",
      "2021-07-26 12:13:11,835 - INFO - allennlp.common.params - model.excerpt_encoder.hidden_size = 50\n",
      "2021-07-26 12:13:11,836 - INFO - allennlp.common.params - model.excerpt_encoder.num_layers = 1\n",
      "2021-07-26 12:13:11,836 - INFO - allennlp.common.params - model.excerpt_encoder.bias = True\n",
      "2021-07-26 12:13:11,837 - INFO - allennlp.common.params - model.excerpt_encoder.dropout = 0.0\n",
      "2021-07-26 12:13:11,838 - INFO - allennlp.common.params - model.excerpt_encoder.bidirectional = False\n",
      "2021-07-26 12:13:11,840 - INFO - filelock - Lock 140127918889664 acquired on ./serialization/3/vocabulary/.lock\n",
      "2021-07-26 12:13:11,921 - INFO - filelock - Lock 140127918889664 released on ./serialization/3/vocabulary/.lock\n",
      "2021-07-26 12:13:12,205 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
      "2021-07-26 12:13:12,207 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
      "2021-07-26 12:13:12,207 - INFO - allennlp.common.params - trainer.distributed = False\n",
      "2021-07-26 12:13:12,208 - INFO - allennlp.common.params - trainer.world_size = 1\n",
      "2021-07-26 12:13:12,209 - INFO - allennlp.common.params - trainer.patience = None\n",
      "2021-07-26 12:13:12,210 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n",
      "2021-07-26 12:13:12,211 - INFO - allennlp.common.params - trainer.num_epochs = 1\n",
      "2021-07-26 12:13:12,211 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
      "2021-07-26 12:13:12,212 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
      "2021-07-26 12:13:12,212 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
      "2021-07-26 12:13:12,222 - INFO - allennlp.common.params - trainer.use_amp = False\n",
      "2021-07-26 12:13:12,223 - INFO - allennlp.common.params - trainer.no_grad = None\n",
      "2021-07-26 12:13:12,224 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None\n",
      "2021-07-26 12:13:12,226 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
      "2021-07-26 12:13:12,228 - INFO - allennlp.common.params - trainer.moving_average = None\n",
      "2021-07-26 12:13:12,228 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f7218c400d0>\n",
      "2021-07-26 12:13:12,229 - INFO - allennlp.common.params - trainer.callbacks = None\n",
      "2021-07-26 12:13:12,230 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
      "2021-07-26 12:13:12,231 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
      "2021-07-26 12:13:12,233 - INFO - allennlp.common.params - trainer.optimizer.type = adam\n",
      "2021-07-26 12:13:12,236 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
      "2021-07-26 12:13:12,237 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05\n",
      "2021-07-26 12:13:12,238 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
      "2021-07-26 12:13:12,239 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
      "2021-07-26 12:13:12,240 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0\n",
      "2021-07-26 12:13:12,241 - INFO - allennlp.common.params - trainer.optimizer.amsgrad = False\n",
      "2021-07-26 12:13:12,241 - INFO - allennlp.training.optimizers - Number of trainable parameters: 2744251\n",
      "2021-07-26 12:13:12,243 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
      "2021-07-26 12:13:12,244 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
      "2021-07-26 12:13:12,245 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.weight\n",
      "2021-07-26 12:13:12,246 - INFO - allennlp.common.util - excerpt_encoder._module.weight_ih_l0\n",
      "2021-07-26 12:13:12,248 - INFO - allennlp.common.util - excerpt_encoder._module.weight_hh_l0\n",
      "2021-07-26 12:13:12,249 - INFO - allennlp.common.util - excerpt_encoder._module.bias_ih_l0\n",
      "2021-07-26 12:13:12,250 - INFO - allennlp.common.util - excerpt_encoder._module.bias_hh_l0\n",
      "2021-07-26 12:13:12,251 - INFO - allennlp.common.util - dense.weight\n",
      "2021-07-26 12:13:12,252 - INFO - allennlp.common.util - dense.bias\n",
      "2021-07-26 12:13:12,254 - INFO - allennlp.common.params - type = default\n",
      "2021-07-26 12:13:12,255 - INFO - allennlp.common.params - save_completed_epochs = True\n",
      "2021-07-26 12:13:12,256 - INFO - allennlp.common.params - save_every_num_seconds = None\n",
      "2021-07-26 12:13:12,258 - INFO - allennlp.common.params - save_every_num_batches = None\n",
      "2021-07-26 12:13:12,259 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n",
      "2021-07-26 12:13:12,260 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n",
      "2021-07-26 12:13:12,261 - WARNING - allennlp.training.gradient_descent_trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "2021-07-26 12:13:12,262 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
      "2021-07-26 12:13:12,263 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/0\n",
      "2021-07-26 12:13:12,264 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 648M\n",
      "2021-07-26 12:13:12,265 - INFO - allennlp.training.gradient_descent_trainer - Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0473f28fa7c40d18d971bcf0a454538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 12:13:12,423 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n",
      "2021-07-26 12:13:12,426 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/tokens (Shape: 16 x 198)\n",
      "tensor([[   81,  1452,  1228,  ...,     0,     0,     0],\n",
      "        [  181,     6,    90,  ...,     0,     0,     0],\n",
      "        [  155,    40,  2439,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [46411,     2,   371,  ...,     0,     0,     0],\n",
      "        [   76, 49403, 22407,  ..., 49408,     4, 14702],\n",
      "        [  154,    95,    27,  ...,     0,     0,     0]])\n",
      "2021-07-26 12:13:12,430 - INFO - allennlp.training.callbacks.console_logger - batch_input/target (Shape: 16)\n",
      "tensor([-0.1954, -0.8521, -2.0287,  ..., -3.1648, -0.8207, -1.7276])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18440/4143978547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m allennlp.commands.train.train_model_from_file(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mparameter_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./training_configs/baseline.jsonnet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mserialization_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./serialization/3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/commands/train.py\u001b[0m in \u001b[0;36mtrain_model_from_file\u001b[0;34m(parameter_filename, serialization_dir, overrides, recover, force, node_rank, include_package, dry_run, file_friendly_logging)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Load the experiment config from a file and pass it to `train_model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     return train_model(\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mserialization_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialization_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/commands/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, serialization_dir, recover, force, node_rank, include_package, dry_run, file_friendly_logging)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;31m# one cuda device, we just run a single training process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdistributed_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         model = _train_worker(\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mprocess_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/commands/train.py\u001b[0m in \u001b[0;36m_train_worker\u001b[0;34m(process_rank, params, serialization_dir, include_package, dry_run, node_rank, primary_addr, primary_port, world_size, distributed_device_ids, file_friendly_logging, include_in_archive)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# if we have completed an epoch, try to create a model archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/commands/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36m_try_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs_completed\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_after_epochs_completed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    480\u001b[0m                         )\n\u001b[1;32m    481\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_group_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/github.com/himkt/commonlitreadabilityprize/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "allennlp.commands.train.train_model_from_file(\n",
    "    parameter_filename=\"./training_configs/baseline.jsonnet\",\n",
    "    serialization_dir=\"./serialization/3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from commonlitreadabilityprize.predictors import RegressorPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = load_archive(\"serialization/2\")\n",
    "predictor = RegressorPredictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pandas.read_csv(\"data/test.csv\")\n",
    "print(test_df.head())\n",
    "\n",
    "batch_json = test_df.excerpt.apply(lambda excerpt: {\"excerpt\": excerpt}).tolist()\n",
    "predictor.predict_batch_json(batch_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIterator:\n",
    "        def __init__(self, data, batch_size):\n",
    "                self.data = data\n",
    "                self.batch_size = batch_size\n",
    "                self.cur = 0\n",
    "            \n",
    "        def __iter__(self):\n",
    "                return self\n",
    "            \n",
    "        def __next__(self):\n",
    "                batch = self.data[self.cur:self.cur+self.batch_size]\n",
    "                self.cur += self.batch_size\n",
    "                if len(batch) == 0:\n",
    "                    raise StopIteration\n",
    "                return batch\n",
    "\n",
    "\n",
    "predictions = []\n",
    "batch_iterator = BatchIterator(batch_json, batch_size=2)\n",
    "\n",
    "for batch in batch_iterator:\n",
    "    predictions += predictor.predict_batch_json(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4602897",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"target\"] = list(map(lambda p: p[\"logit\"][0], predictions))\n",
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
