{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T05:46:06.382819Z","iopub.execute_input":"2021-07-27T05:46:06.383343Z","iopub.status.idle":"2021-07-27T05:46:06.404855Z","shell.execute_reply.started":"2021-07-27T05:46:06.383251Z","shell.execute_reply":"2021-07-27T05:46:06.403731Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/input/roberta-base/config.json\n/kaggle/input/roberta-base/merges.txt\n/kaggle/input/roberta-base/vocab.json\n/kaggle/input/roberta-base/pytorch_model.bin\n/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n/kaggle/input/commonlitreadabilityprize/train.csv\n/kaggle/input/commonlitreadabilityprize/test.csv\n/kaggle/input/robertalarge/config.json\n/kaggle/input/robertalarge/merges.txt\n/kaggle/input/robertalarge/vocab.json\n/kaggle/input/robertalarge/pytorch_model.bin\n/kaggle/input/robertalarge/modelcard.json\n/kaggle/input/glove-vec/glove.6B.100d.word2vec\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas\nimport numpy\n\n\ndf = pandas.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nnum_records = len(df)\n\nids = numpy.arange(num_records)\nids = numpy.random.permutation(ids)\n\ntrain_size = 0.8\npartition = int(num_records * train_size)\n\ntrain_ids, valid_ids = ids[:partition], ids[partition:]\n\ndf.loc[train_ids].to_csv(\"./processed_train.csv\", index=False)\ndf.loc[valid_ids].to_csv(\"./processed_valid.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:46:06.408202Z","iopub.execute_input":"2021-07-27T05:46:06.408519Z","iopub.status.idle":"2021-07-27T05:46:06.564634Z","shell.execute_reply.started":"2021-07-27T05:46:06.408491Z","shell.execute_reply":"2021-07-27T05:46:06.563654Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from typing import Any, Dict, Iterable, MutableMapping, Optional\nfrom urllib.parse import urlparse\n\nfrom allennlp.data import DatasetReader\nfrom allennlp.data import Tokenizer\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields import ArrayField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer\nfrom allennlp.data.tokenizers.token_class import Token\nimport pandas\nimport numpy\nfrom overrides import overrides\n\n\n@DatasetReader.register(\"commonlit_reader\")\nclass CommonlitDatasetReader(DatasetReader):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        excerpt_token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n        hostname_token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n    ) -> None:\n\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.excerpt_token_indexers: Dict[str, TokenIndexer] = excerpt_token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n        self.hostname_token_indexers: Dict[str, TokenIndexer] = hostname_token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        instances = []\n\n        dataframe = pandas.read_csv(file_path)\n        dataframe[\"hostname\"] = dataframe \\\n            .url_legal \\\n            .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\n        for _, row in dataframe.iterrows():\n            excerpt = row.excerpt\n            hostname = row.hostname\n            target = row.target if hasattr(row, \"target\") else None\n            instances.append(self.text_to_instance(excerpt, hostname, target))\n\n        return instances\n\n    @overrides\n    def text_to_instance(self, excerpt: str, hostname: str, target: Optional[float] = None) -> Instance:\n        excerpt_tokens = self.tokenizer.tokenize(excerpt)\n        hostname_tokens = [Token(text=hostname)]\n        fields: MutableMapping[str, Field[Any]] = {\n            \"excerpt\": TextField(excerpt_tokens),\n            \"hostname\": TextField(hostname_tokens),\n        }\n        if target is not None:\n            fields[\"target\"] = ArrayField(numpy.asarray(target, dtype=numpy.float32))\n        return Instance(fields=fields)\n\n    def apply_token_indexers(self, instance: Instance) -> None:\n        assert isinstance(instance.fields[\"excerpt\"], TextField)\n        instance.fields[\"excerpt\"].token_indexers = self.excerpt_token_indexers\n        assert isinstance(instance.fields[\"hostname\"], TextField)\n        instance.fields[\"hostname\"].token_indexers = self.hostname_token_indexers\n\n\nfrom typing import Dict, Optional\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules import TextFieldEmbedder\nfrom allennlp.modules import Seq2VecEncoder\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.data.fields.text_field import TextFieldTensors\nfrom overrides.overrides import overrides\nfrom torch import FloatTensor\nfrom torch.functional import Tensor\nfrom torch.nn.functional import mse_loss\nfrom torch import cat\nfrom torch import sqrt\nfrom torch.nn import Linear\n\n\nEPS = 1e-8\n\n\n@Model.register(\"naive\")\nclass NaiveRegressor(Model):\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        excerpt_embedder: TextFieldEmbedder,\n        excerpt_encoder: Seq2VecEncoder,\n        hostname_embedder: Optional[TextFieldEmbedder] = None,\n    ) -> None:\n\n        super().__init__(vocab)\n\n        self.vocab = vocab\n        self.excerpt_embedder = excerpt_embedder\n        self.excerpt_encoder = excerpt_encoder\n        self.hostname_embedder = hostname_embedder\n\n        in_features = self.excerpt_encoder.get_output_dim()\n        if hostname_embedder is not None:\n            in_features += hostname_embedder.get_output_dim()\n\n        self.classification_layer = Linear(\n            in_features=in_features,\n            out_features=1,\n        )\n\n    @overrides\n    def forward(\n        self,\n        excerpt: TextFieldTensors,\n        hostname: Optional[TextFieldTensors] = None,\n        target: Optional[FloatTensor] = None,\n    ) -> Dict[str, Tensor]:\n\n        mask = get_text_field_mask(excerpt)\n        excerpt_emb = self.excerpt_embedder(excerpt)\n        hidden_state = self.excerpt_encoder(excerpt_emb, mask=mask)\n\n        if self.hostname_embedder is not None and hostname is not None:\n            hostname_emb = self.hostname_embedder(hostname)\n            hidden_state = cat((hidden_state, hostname_emb.squeeze(dim=1)), dim=1)\n\n        logit = self.classification_layer(hidden_state)\n\n        output_dict = {\"logit\": logit}\n        if target is not None:\n            output_dict[\"loss\"] = sqrt(mse_loss(logit.view(-1), target) + EPS)\n\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {}\n\n\nfrom allennlp.common.util import JsonDict\nfrom allennlp.data.instance import Instance\nfrom allennlp.predictors import Predictor\n\n\n@Predictor.register(\"regressor_predictor\")\nclass RegressorPredictor(Predictor):\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        return self._dataset_reader.text_to_instance(**json_dict)  # type: ignore\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:46:06.566878Z","iopub.execute_input":"2021-07-27T05:46:06.567325Z","iopub.status.idle":"2021-07-27T05:46:06.609791Z","shell.execute_reply.started":"2021-07-27T05:46:06.567282Z","shell.execute_reply":"2021-07-27T05:46:06.608814Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!ls ../input","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:46:06.613166Z","iopub.execute_input":"2021-07-27T05:46:06.613475Z","iopub.status.idle":"2021-07-27T05:46:07.402242Z","shell.execute_reply.started":"2021-07-27T05:46:06.613446Z","shell.execute_reply":"2021-07-27T05:46:07.401079Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"commonlitreadabilityprize  glove-vec  roberta-base  robertalarge\n","output_type":"stream"}]},{"cell_type":"code","source":"jsonnet_text = \"\"\"\\\n{\n    dataset_reader: {\n        type: \"commonlit_reader\",\n        tokenizer: {\n            type: \"pretrained_transformer\",\n            model_name: \"../input/robertalarge\",\n        },\n        excerpt_token_indexers: {\n            tokens: {\n                type: \"pretrained_transformer\",\n                model_name: \"../input/robertalarge\",\n            },\n        },\n    },\n    train_data_path: \"./processed_train.csv\",\n    validation_data_path: \"./processed_valid.csv\",\n    model: {\n        type: \"naive\",\n        excerpt_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    type: \"pretrained_transformer\",\n                    model_name: \"../input/robertalarge\",\n                },\n            },\n        },\n        excerpt_encoder: {\n            type: \"bert_pooler\",\n            pretrained_model: \"../input/robertalarge\",\n        },\n        hostname_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    embedding_dim: 50,\n                },\n            },\n        },\n    },\n    trainer: {\n        num_epochs: 15,\n        learning_rate_scheduler: {\n            type: \"slanted_triangular\",\n            num_epochs: 10,\n            num_steps_per_epoch: 3088,\n            cut_frac: 0.06\n        },\n        optimizer: {\n            type: \"huggingface_adamw\",\n            lr: 5e-7,\n            weight_decay: 0.05,\n        },\n        validation_metric: \"-loss\"\n    },\n    data_loader: {\n        batch_size: 8,\n        shuffle: true\n    }\n}\n\"\"\"\n\nf = open(\"baseline.jsonnet\", \"w\")\nf.write(jsonnet_text)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:46:07.404435Z","iopub.execute_input":"2021-07-27T05:46:07.404809Z","iopub.status.idle":"2021-07-27T05:46:07.414865Z","shell.execute_reply.started":"2021-07-27T05:46:07.404743Z","shell.execute_reply":"2021-07-27T05:46:07.411855Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import allennlp.commands\n\nallennlp.commands.train.train_model_from_file(\n    parameter_filename=\"./baseline.jsonnet\",\n    serialization_dir=\"./serialization/1\",\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:46:07.416850Z","iopub.execute_input":"2021-07-27T05:46:07.417303Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"2021-07-27 05:46:10,427 - INFO - allennlp.common.params - random_seed = 13370\n2021-07-27 05:46:10,428 - INFO - allennlp.common.params - numpy_seed = 1337\n2021-07-27 05:46:10,430 - INFO - allennlp.common.params - pytorch_seed = 133\n2021-07-27 05:46:10,486 - INFO - allennlp.common.checks - Pytorch version: 1.7.0\n2021-07-27 05:46:10,487 - INFO - allennlp.common.params - type = default\n2021-07-27 05:46:10,490 - INFO - allennlp.common.params - dataset_reader.type = commonlit_reader\n2021-07-27 05:46:10,493 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n2021-07-27 05:46:10,494 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = ../input/robertalarge\n2021-07-27 05:46:10,495 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n2021-07-27 05:46:10,497 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n2021-07-27 05:46:10,501 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n2021-07-27 05:46:11,330 - INFO - allennlp.common.params - dataset_reader.excerpt_token_indexers.tokens.type = pretrained_transformer\n2021-07-27 05:46:11,331 - INFO - allennlp.common.params - dataset_reader.excerpt_token_indexers.tokens.token_min_padding_length = 0\n2021-07-27 05:46:11,332 - INFO - allennlp.common.params - dataset_reader.excerpt_token_indexers.tokens.model_name = ../input/robertalarge\n2021-07-27 05:46:11,336 - INFO - allennlp.common.params - dataset_reader.excerpt_token_indexers.tokens.namespace = tags\n2021-07-27 05:46:11,337 - INFO - allennlp.common.params - dataset_reader.excerpt_token_indexers.tokens.max_length = None\n2021-07-27 05:46:11,339 - INFO - allennlp.common.params - dataset_reader.excerpt_token_indexers.tokens.tokenizer_kwargs = None\n2021-07-27 05:46:11,345 - INFO - allennlp.common.params - dataset_reader.hostname_token_indexers = None\n2021-07-27 05:46:11,346 - INFO - allennlp.common.params - train_data_path = ./processed_train.csv\n2021-07-27 05:46:11,348 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7fda3912be90>\n2021-07-27 05:46:11,350 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n2021-07-27 05:46:11,355 - INFO - allennlp.common.params - validation_dataset_reader = None\n2021-07-27 05:46:11,356 - INFO - allennlp.common.params - validation_data_path = ./processed_valid.csv\n2021-07-27 05:46:11,357 - INFO - allennlp.common.params - validation_data_loader = None\n2021-07-27 05:46:11,358 - INFO - allennlp.common.params - test_data_path = None\n2021-07-27 05:46:11,365 - INFO - allennlp.common.params - evaluate_on_test = False\n2021-07-27 05:46:11,366 - INFO - allennlp.common.params - batch_weight_key = \n2021-07-27 05:46:11,368 - INFO - allennlp.common.params - data_loader.type = multiprocess\n2021-07-27 05:46:11,370 - INFO - allennlp.common.params - data_loader.batch_size = 8\n2021-07-27 05:46:11,371 - INFO - allennlp.common.params - data_loader.drop_last = False\n2021-07-27 05:46:11,373 - INFO - allennlp.common.params - data_loader.shuffle = True\n2021-07-27 05:46:11,374 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n2021-07-27 05:46:11,381 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n2021-07-27 05:46:11,382 - INFO - allennlp.common.params - data_loader.num_workers = 0\n2021-07-27 05:46:11,383 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n2021-07-27 05:46:11,385 - INFO - allennlp.common.params - data_loader.start_method = fork\n2021-07-27 05:46:11,386 - INFO - allennlp.common.params - data_loader.cuda_device = None\n2021-07-27 05:46:11,387 - INFO - allennlp.common.params - data_loader.quiet = False\n2021-07-27 05:46:11,388 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7fda8d1d0210>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"loading instances: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bba9a97a9140eb8aab600283163b94"}},"metadata":{}},{"name":"stdout","text":"2021-07-27 05:46:17,048 - INFO - allennlp.common.params - data_loader.type = multiprocess\n2021-07-27 05:46:17,050 - INFO - allennlp.common.params - data_loader.batch_size = 8\n2021-07-27 05:46:17,057 - INFO - allennlp.common.params - data_loader.drop_last = False\n2021-07-27 05:46:17,058 - INFO - allennlp.common.params - data_loader.shuffle = True\n2021-07-27 05:46:17,060 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n2021-07-27 05:46:17,061 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n2021-07-27 05:46:17,063 - INFO - allennlp.common.params - data_loader.num_workers = 0\n2021-07-27 05:46:17,064 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n2021-07-27 05:46:17,065 - INFO - allennlp.common.params - data_loader.start_method = fork\n2021-07-27 05:46:17,066 - INFO - allennlp.common.params - data_loader.cuda_device = None\n2021-07-27 05:46:17,067 - INFO - allennlp.common.params - data_loader.quiet = False\n2021-07-27 05:46:17,069 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7fda8d1d0210>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"loading instances: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f42293a7b31424c89bf0d5363265925"}},"metadata":{}},{"name":"stdout","text":"2021-07-27 05:46:18,316 - INFO - allennlp.common.params - type = from_instances\n2021-07-27 05:46:18,318 - INFO - allennlp.common.params - min_count = None\n2021-07-27 05:46:18,319 - INFO - allennlp.common.params - max_vocab_size = None\n2021-07-27 05:46:18,320 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n2021-07-27 05:46:18,321 - INFO - allennlp.common.params - pretrained_files = None\n2021-07-27 05:46:18,322 - INFO - allennlp.common.params - only_include_pretrained_words = False\n2021-07-27 05:46:18,325 - INFO - allennlp.common.params - tokens_to_add = None\n2021-07-27 05:46:18,333 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n2021-07-27 05:46:18,334 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n2021-07-27 05:46:18,336 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n2021-07-27 05:46:18,337 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"building vocab: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b37f6c7025f456896ca8ed0d9a8f9a5"}},"metadata":{}},{"name":"stdout","text":"2021-07-27 05:46:18,521 - INFO - allennlp.common.params - model.type = naive\n2021-07-27 05:46:18,523 - INFO - allennlp.common.params - model.excerpt_embedder.type = basic\n2021-07-27 05:46:18,525 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.type = pretrained_transformer\n2021-07-27 05:46:18,527 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.model_name = ../input/robertalarge\n2021-07-27 05:46:18,528 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.max_length = None\n2021-07-27 05:46:18,530 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.sub_module = None\n2021-07-27 05:46:18,531 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.train_parameters = True\n2021-07-27 05:46:18,537 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.eval_mode = False\n2021-07-27 05:46:18,538 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.last_layer_only = True\n2021-07-27 05:46:18,540 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.override_weights_file = None\n2021-07-27 05:46:18,542 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n2021-07-27 05:46:18,543 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.load_weights = True\n2021-07-27 05:46:18,545 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.gradient_checkpointing = None\n2021-07-27 05:46:18,546 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.tokenizer_kwargs = None\n2021-07-27 05:46:18,547 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.transformer_kwargs = None\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"2021-07-27 05:46:38,523 - INFO - allennlp.common.params - model.excerpt_encoder.type = bert_pooler\n2021-07-27 05:46:38,524 - INFO - allennlp.common.params - model.excerpt_encoder.pretrained_model = ../input/robertalarge\n2021-07-27 05:46:38,526 - INFO - allennlp.common.params - model.excerpt_encoder.override_weights_file = None\n2021-07-27 05:46:38,530 - INFO - allennlp.common.params - model.excerpt_encoder.override_weights_strip_prefix = None\n2021-07-27 05:46:38,531 - INFO - allennlp.common.params - model.excerpt_encoder.load_weights = True\n2021-07-27 05:46:38,532 - INFO - allennlp.common.params - model.excerpt_encoder.requires_grad = True\n2021-07-27 05:46:38,533 - INFO - allennlp.common.params - model.excerpt_encoder.dropout = 0.0\n2021-07-27 05:46:38,534 - INFO - allennlp.common.params - model.excerpt_encoder.transformer_kwargs = None\n2021-07-27 05:46:38,539 - INFO - allennlp.common.params - model.hostname_embedder.type = basic\n2021-07-27 05:46:38,542 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.type = embedding\n2021-07-27 05:46:38,543 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.embedding_dim = 50\n2021-07-27 05:46:38,544 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.num_embeddings = None\n2021-07-27 05:46:38,548 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.projection_dim = None\n2021-07-27 05:46:38,553 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.weight = None\n2021-07-27 05:46:38,554 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.padding_index = None\n2021-07-27 05:46:38,556 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.trainable = True\n2021-07-27 05:46:38,557 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.max_norm = None\n2021-07-27 05:46:38,559 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.norm_type = 2.0\n2021-07-27 05:46:38,560 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.scale_grad_by_freq = False\n2021-07-27 05:46:38,561 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.sparse = False\n2021-07-27 05:46:38,567 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.vocab_namespace = tokens\n2021-07-27 05:46:38,568 - INFO - allennlp.common.params - model.hostname_embedder.token_embedders.tokens.pretrained_file = None\n2021-07-27 05:46:38,571 - INFO - filelock - Lock 140574383494288 acquired on ./serialization/1/vocabulary/.lock\n2021-07-27 05:46:38,572 - INFO - filelock - Lock 140574383494288 released on ./serialization/1/vocabulary/.lock\n2021-07-27 05:46:38,911 - INFO - allennlp.common.params - trainer.type = gradient_descent\n2021-07-27 05:46:38,913 - INFO - allennlp.common.params - trainer.cuda_device = None\n2021-07-27 05:46:38,914 - INFO - allennlp.common.params - trainer.distributed = False\n2021-07-27 05:46:38,916 - INFO - allennlp.common.params - trainer.world_size = 1\n2021-07-27 05:46:38,917 - INFO - allennlp.common.params - trainer.patience = None\n2021-07-27 05:46:38,917 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n2021-07-27 05:46:38,923 - INFO - allennlp.common.params - trainer.num_epochs = 15\n2021-07-27 05:46:38,925 - INFO - allennlp.common.params - trainer.grad_norm = None\n2021-07-27 05:46:38,926 - INFO - allennlp.common.params - trainer.grad_clipping = None\n2021-07-27 05:46:38,927 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n2021-07-27 05:46:38,929 - INFO - allennlp.common.params - trainer.use_amp = False\n2021-07-27 05:46:38,930 - INFO - allennlp.common.params - trainer.no_grad = None\n2021-07-27 05:46:38,932 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n2021-07-27 05:46:38,933 - INFO - allennlp.common.params - trainer.moving_average = None\n2021-07-27 05:46:38,940 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7fda3a76ed50>\n2021-07-27 05:46:38,941 - INFO - allennlp.common.params - trainer.callbacks = None\n2021-07-27 05:46:38,942 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n2021-07-27 05:46:38,943 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n2021-07-27 05:46:46,100 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n2021-07-27 05:46:46,101 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n2021-07-27 05:46:46,103 - INFO - allennlp.common.params - trainer.optimizer.lr = 5e-07\n2021-07-27 05:46:46,105 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n2021-07-27 05:46:46,107 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n2021-07-27 05:46:46,108 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.05\n2021-07-27 05:46:46,110 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n2021-07-27 05:46:46,112 - INFO - allennlp.training.optimizers - Number of trainable parameters: 356411319\n2021-07-27 05:46:46,118 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n2021-07-27 05:46:46,124 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n2021-07-27 05:46:46,125 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n2021-07-27 05:46:46,126 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n2021-07-27 05:46:46,127 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n2021-07-27 05:46:46,128 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n2021-07-27 05:46:46,129 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n2021-07-27 05:46:46,135 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n2021-07-27 05:46:46,140 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n2021-07-27 05:46:46,141 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n2021-07-27 05:46:46,143 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n2021-07-27 05:46:46,148 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n2021-07-27 05:46:46,149 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n2021-07-27 05:46:46,150 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n2021-07-27 05:46:46,152 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n2021-07-27 05:46:46,153 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,155 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,156 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n2021-07-27 05:46:46,157 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n2021-07-27 05:46:46,159 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n2021-07-27 05:46:46,160 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n2021-07-27 05:46:46,162 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n2021-07-27 05:46:46,163 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n2021-07-27 05:46:46,164 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n2021-07-27 05:46:46,166 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n2021-07-27 05:46:46,167 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n2021-07-27 05:46:46,168 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n2021-07-27 05:46:46,170 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n2021-07-27 05:46:46,171 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n2021-07-27 05:46:46,173 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n2021-07-27 05:46:46,174 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n2021-07-27 05:46:46,175 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,177 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,188 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n2021-07-27 05:46:46,189 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n2021-07-27 05:46:46,191 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n2021-07-27 05:46:46,192 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n2021-07-27 05:46:46,193 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n2021-07-27 05:46:46,195 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n2021-07-27 05:46:46,197 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n2021-07-27 05:46:46,198 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n2021-07-27 05:46:46,199 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n2021-07-27 05:46:46,200 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n2021-07-27 05:46:46,201 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n2021-07-27 05:46:46,202 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n2021-07-27 05:46:46,203 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n2021-07-27 05:46:46,204 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n2021-07-27 05:46:46,205 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,206 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,207 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n2021-07-27 05:46:46,208 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n2021-07-27 05:46:46,209 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n2021-07-27 05:46:46,210 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n2021-07-27 05:46:46,211 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n2021-07-27 05:46:46,212 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n2021-07-27 05:46:46,213 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n2021-07-27 05:46:46,214 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n2021-07-27 05:46:46,215 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n2021-07-27 05:46:46,216 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n2021-07-27 05:46:46,217 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n2021-07-27 05:46:46,218 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n2021-07-27 05:46:46,219 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n2021-07-27 05:46:46,220 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n2021-07-27 05:46:46,221 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,222 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,223 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n2021-07-27 05:46:46,224 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n2021-07-27 05:46:46,225 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n2021-07-27 05:46:46,226 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n2021-07-27 05:46:46,227 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n2021-07-27 05:46:46,228 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n2021-07-27 05:46:46,229 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n2021-07-27 05:46:46,230 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n2021-07-27 05:46:46,231 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n2021-07-27 05:46:46,232 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n2021-07-27 05:46:46,233 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n2021-07-27 05:46:46,234 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n2021-07-27 05:46:46,235 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n2021-07-27 05:46:46,236 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n2021-07-27 05:46:46,237 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,238 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,239 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n2021-07-27 05:46:46,240 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n2021-07-27 05:46:46,241 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n2021-07-27 05:46:46,242 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n2021-07-27 05:46:46,243 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n2021-07-27 05:46:46,244 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n2021-07-27 05:46:46,245 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n2021-07-27 05:46:46,246 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n2021-07-27 05:46:46,247 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n2021-07-27 05:46:46,248 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n2021-07-27 05:46:46,249 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n2021-07-27 05:46:46,250 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n2021-07-27 05:46:46,251 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n2021-07-27 05:46:46,252 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n2021-07-27 05:46:46,253 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,254 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,255 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n2021-07-27 05:46:46,256 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n2021-07-27 05:46:46,257 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n2021-07-27 05:46:46,258 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n2021-07-27 05:46:46,259 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n2021-07-27 05:46:46,260 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n2021-07-27 05:46:46,262 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n2021-07-27 05:46:46,263 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n2021-07-27 05:46:46,264 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n2021-07-27 05:46:46,265 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n2021-07-27 05:46:46,266 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n2021-07-27 05:46:46,267 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n2021-07-27 05:46:46,268 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n2021-07-27 05:46:46,269 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n2021-07-27 05:46:46,270 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,271 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,272 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n2021-07-27 05:46:46,273 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n2021-07-27 05:46:46,274 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n2021-07-27 05:46:46,276 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n2021-07-27 05:46:46,277 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n2021-07-27 05:46:46,278 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n2021-07-27 05:46:46,279 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n2021-07-27 05:46:46,280 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n2021-07-27 05:46:46,281 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n2021-07-27 05:46:46,282 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n2021-07-27 05:46:46,283 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n2021-07-27 05:46:46,284 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n2021-07-27 05:46:46,286 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n2021-07-27 05:46:46,287 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n2021-07-27 05:46:46,288 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,289 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,290 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n2021-07-27 05:46:46,291 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n2021-07-27 05:46:46,292 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n2021-07-27 05:46:46,293 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n2021-07-27 05:46:46,294 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n2021-07-27 05:46:46,295 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n2021-07-27 05:46:46,296 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n2021-07-27 05:46:46,297 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n2021-07-27 05:46:46,298 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n2021-07-27 05:46:46,299 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n2021-07-27 05:46:46,300 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n2021-07-27 05:46:46,302 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n2021-07-27 05:46:46,303 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n2021-07-27 05:46:46,304 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n2021-07-27 05:46:46,305 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,306 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,307 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n2021-07-27 05:46:46,308 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n2021-07-27 05:46:46,309 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n2021-07-27 05:46:46,310 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n2021-07-27 05:46:46,311 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n2021-07-27 05:46:46,312 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n2021-07-27 05:46:46,313 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n2021-07-27 05:46:46,314 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n2021-07-27 05:46:46,315 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n2021-07-27 05:46:46,317 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n2021-07-27 05:46:46,318 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n2021-07-27 05:46:46,319 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n2021-07-27 05:46:46,320 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n2021-07-27 05:46:46,321 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n2021-07-27 05:46:46,322 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,323 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,325 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n2021-07-27 05:46:46,326 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n2021-07-27 05:46:46,327 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n2021-07-27 05:46:46,328 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n2021-07-27 05:46:46,329 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n2021-07-27 05:46:46,331 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n2021-07-27 05:46:46,332 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n2021-07-27 05:46:46,333 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n2021-07-27 05:46:46,334 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n2021-07-27 05:46:46,335 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n2021-07-27 05:46:46,336 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n2021-07-27 05:46:46,337 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n2021-07-27 05:46:46,338 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n2021-07-27 05:46:46,340 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n2021-07-27 05:46:46,341 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,342 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,343 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n2021-07-27 05:46:46,344 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n2021-07-27 05:46:46,345 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n2021-07-27 05:46:46,346 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n2021-07-27 05:46:46,347 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n2021-07-27 05:46:46,348 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n2021-07-27 05:46:46,350 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n2021-07-27 05:46:46,351 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n2021-07-27 05:46:46,352 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n2021-07-27 05:46:46,353 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n2021-07-27 05:46:46,354 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n2021-07-27 05:46:46,356 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n2021-07-27 05:46:46,357 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n2021-07-27 05:46:46,358 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n2021-07-27 05:46:46,359 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,360 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,362 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n2021-07-27 05:46:46,363 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n2021-07-27 05:46:46,364 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n2021-07-27 05:46:46,365 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n2021-07-27 05:46:46,366 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n2021-07-27 05:46:46,368 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n2021-07-27 05:46:46,369 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.query.weight\n2021-07-27 05:46:46,370 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.query.bias\n2021-07-27 05:46:46,371 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.key.weight\n2021-07-27 05:46:46,372 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.key.bias\n2021-07-27 05:46:46,374 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.value.weight\n2021-07-27 05:46:46,375 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.self.value.bias\n2021-07-27 05:46:46,376 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.dense.weight\n2021-07-27 05:46:46,377 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.dense.bias\n2021-07-27 05:46:46,378 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,379 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,381 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.intermediate.dense.weight\n2021-07-27 05:46:46,382 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.intermediate.dense.bias\n2021-07-27 05:46:46,383 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.dense.weight\n2021-07-27 05:46:46,384 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.dense.bias\n2021-07-27 05:46:46,385 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.LayerNorm.weight\n2021-07-27 05:46:46,386 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.12.output.LayerNorm.bias\n2021-07-27 05:46:46,387 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.query.weight\n2021-07-27 05:46:46,388 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.query.bias\n2021-07-27 05:46:46,390 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.key.weight\n2021-07-27 05:46:46,391 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.key.bias\n2021-07-27 05:46:46,392 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.value.weight\n2021-07-27 05:46:46,393 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.self.value.bias\n2021-07-27 05:46:46,394 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.dense.weight\n2021-07-27 05:46:46,396 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.dense.bias\n2021-07-27 05:46:46,397 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,398 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,399 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.intermediate.dense.weight\n2021-07-27 05:46:46,400 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.intermediate.dense.bias\n2021-07-27 05:46:46,402 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.dense.weight\n2021-07-27 05:46:46,403 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.dense.bias\n2021-07-27 05:46:46,404 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.LayerNorm.weight\n2021-07-27 05:46:46,405 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.13.output.LayerNorm.bias\n2021-07-27 05:46:46,406 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.query.weight\n2021-07-27 05:46:46,407 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.query.bias\n2021-07-27 05:46:46,409 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.key.weight\n2021-07-27 05:46:46,410 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.key.bias\n2021-07-27 05:46:46,411 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.value.weight\n2021-07-27 05:46:46,412 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.self.value.bias\n2021-07-27 05:46:46,413 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.dense.weight\n2021-07-27 05:46:46,414 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.dense.bias\n2021-07-27 05:46:46,415 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,417 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,418 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.intermediate.dense.weight\n2021-07-27 05:46:46,419 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.intermediate.dense.bias\n2021-07-27 05:46:46,420 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.dense.weight\n2021-07-27 05:46:46,421 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.dense.bias\n2021-07-27 05:46:46,422 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.LayerNorm.weight\n2021-07-27 05:46:46,423 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.14.output.LayerNorm.bias\n2021-07-27 05:46:46,424 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.query.weight\n2021-07-27 05:46:46,426 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.query.bias\n2021-07-27 05:46:46,427 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.key.weight\n2021-07-27 05:46:46,428 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.key.bias\n2021-07-27 05:46:46,429 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.value.weight\n2021-07-27 05:46:46,430 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.self.value.bias\n2021-07-27 05:46:46,431 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.dense.weight\n2021-07-27 05:46:46,432 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.dense.bias\n2021-07-27 05:46:46,434 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,435 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,436 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.intermediate.dense.weight\n2021-07-27 05:46:46,437 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.intermediate.dense.bias\n2021-07-27 05:46:46,438 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.dense.weight\n2021-07-27 05:46:46,440 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.dense.bias\n2021-07-27 05:46:46,441 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.LayerNorm.weight\n2021-07-27 05:46:46,442 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.15.output.LayerNorm.bias\n2021-07-27 05:46:46,443 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.query.weight\n2021-07-27 05:46:46,444 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.query.bias\n2021-07-27 05:46:46,445 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.key.weight\n2021-07-27 05:46:46,447 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.key.bias\n2021-07-27 05:46:46,448 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.value.weight\n2021-07-27 05:46:46,449 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.self.value.bias\n2021-07-27 05:46:46,450 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.dense.weight\n2021-07-27 05:46:46,451 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.dense.bias\n2021-07-27 05:46:46,452 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,454 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,455 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.intermediate.dense.weight\n2021-07-27 05:46:46,456 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.intermediate.dense.bias\n2021-07-27 05:46:46,458 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.output.dense.weight\n2021-07-27 05:46:46,459 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.output.dense.bias\n2021-07-27 05:46:46,460 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.output.LayerNorm.weight\n2021-07-27 05:46:46,462 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.16.output.LayerNorm.bias\n2021-07-27 05:46:46,463 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.self.query.weight\n2021-07-27 05:46:46,464 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.self.query.bias\n2021-07-27 05:46:46,465 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.self.key.weight\n2021-07-27 05:46:46,466 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.self.key.bias\n2021-07-27 05:46:46,467 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.self.value.weight\n2021-07-27 05:46:46,469 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.self.value.bias\n2021-07-27 05:46:46,470 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.output.dense.weight\n2021-07-27 05:46:46,471 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.output.dense.bias\n2021-07-27 05:46:46,472 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,473 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,474 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.intermediate.dense.weight\n2021-07-27 05:46:46,475 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.intermediate.dense.bias\n2021-07-27 05:46:46,476 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.output.dense.weight\n2021-07-27 05:46:46,478 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.output.dense.bias\n2021-07-27 05:46:46,479 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.output.LayerNorm.weight\n2021-07-27 05:46:46,480 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.17.output.LayerNorm.bias\n2021-07-27 05:46:46,481 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.self.query.weight\n2021-07-27 05:46:46,482 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.self.query.bias\n2021-07-27 05:46:46,483 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.self.key.weight\n2021-07-27 05:46:46,484 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.self.key.bias\n2021-07-27 05:46:46,485 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.self.value.weight\n2021-07-27 05:46:46,487 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.self.value.bias\n2021-07-27 05:46:46,488 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.output.dense.weight\n2021-07-27 05:46:46,489 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.output.dense.bias\n2021-07-27 05:46:46,490 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,491 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,492 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.intermediate.dense.weight\n2021-07-27 05:46:46,494 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.intermediate.dense.bias\n2021-07-27 05:46:46,495 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.output.dense.weight\n2021-07-27 05:46:46,496 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.output.dense.bias\n2021-07-27 05:46:46,497 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.output.LayerNorm.weight\n2021-07-27 05:46:46,498 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.18.output.LayerNorm.bias\n2021-07-27 05:46:46,499 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.self.query.weight\n2021-07-27 05:46:46,500 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.self.query.bias\n2021-07-27 05:46:46,502 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.self.key.weight\n2021-07-27 05:46:46,503 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.self.key.bias\n2021-07-27 05:46:46,504 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.self.value.weight\n2021-07-27 05:46:46,505 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.self.value.bias\n2021-07-27 05:46:46,506 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.output.dense.weight\n2021-07-27 05:46:46,507 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.output.dense.bias\n2021-07-27 05:46:46,508 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,509 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,511 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.intermediate.dense.weight\n2021-07-27 05:46:46,512 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.intermediate.dense.bias\n2021-07-27 05:46:46,513 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.output.dense.weight\n2021-07-27 05:46:46,514 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.output.dense.bias\n2021-07-27 05:46:46,515 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.output.LayerNorm.weight\n2021-07-27 05:46:46,516 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.19.output.LayerNorm.bias\n2021-07-27 05:46:46,518 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.self.query.weight\n2021-07-27 05:46:46,519 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.self.query.bias\n2021-07-27 05:46:46,520 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.self.key.weight\n2021-07-27 05:46:46,521 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.self.key.bias\n2021-07-27 05:46:46,522 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.self.value.weight\n2021-07-27 05:46:46,523 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.self.value.bias\n2021-07-27 05:46:46,524 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.output.dense.weight\n2021-07-27 05:46:46,526 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.output.dense.bias\n2021-07-27 05:46:46,527 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,528 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,529 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.intermediate.dense.weight\n2021-07-27 05:46:46,530 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.intermediate.dense.bias\n2021-07-27 05:46:46,531 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.output.dense.weight\n2021-07-27 05:46:46,533 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.output.dense.bias\n2021-07-27 05:46:46,534 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.output.LayerNorm.weight\n2021-07-27 05:46:46,535 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.20.output.LayerNorm.bias\n2021-07-27 05:46:46,536 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.self.query.weight\n2021-07-27 05:46:46,537 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.self.query.bias\n2021-07-27 05:46:46,538 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.self.key.weight\n2021-07-27 05:46:46,540 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.self.key.bias\n2021-07-27 05:46:46,541 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.self.value.weight\n2021-07-27 05:46:46,542 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.self.value.bias\n2021-07-27 05:46:46,544 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.output.dense.weight\n2021-07-27 05:46:46,545 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.output.dense.bias\n2021-07-27 05:46:46,546 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,547 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,548 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.intermediate.dense.weight\n2021-07-27 05:46:46,549 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.intermediate.dense.bias\n2021-07-27 05:46:46,550 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.output.dense.weight\n2021-07-27 05:46:46,552 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.output.dense.bias\n2021-07-27 05:46:46,553 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.output.LayerNorm.weight\n2021-07-27 05:46:46,554 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.21.output.LayerNorm.bias\n2021-07-27 05:46:46,555 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.self.query.weight\n2021-07-27 05:46:46,556 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.self.query.bias\n2021-07-27 05:46:46,557 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.self.key.weight\n2021-07-27 05:46:46,559 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.self.key.bias\n2021-07-27 05:46:46,560 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.self.value.weight\n2021-07-27 05:46:46,561 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.self.value.bias\n2021-07-27 05:46:46,562 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.output.dense.weight\n2021-07-27 05:46:46,563 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.output.dense.bias\n2021-07-27 05:46:46,564 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,566 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,567 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.intermediate.dense.weight\n2021-07-27 05:46:46,568 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.intermediate.dense.bias\n2021-07-27 05:46:46,569 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.output.dense.weight\n2021-07-27 05:46:46,570 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.output.dense.bias\n2021-07-27 05:46:46,571 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.output.LayerNorm.weight\n2021-07-27 05:46:46,573 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.22.output.LayerNorm.bias\n2021-07-27 05:46:46,574 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.self.query.weight\n2021-07-27 05:46:46,575 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.self.query.bias\n2021-07-27 05:46:46,576 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.self.key.weight\n2021-07-27 05:46:46,577 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.self.key.bias\n2021-07-27 05:46:46,578 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.self.value.weight\n2021-07-27 05:46:46,579 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.self.value.bias\n2021-07-27 05:46:46,581 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.output.dense.weight\n2021-07-27 05:46:46,582 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.output.dense.bias\n2021-07-27 05:46:46,584 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n2021-07-27 05:46:46,585 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n2021-07-27 05:46:46,586 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.intermediate.dense.weight\n2021-07-27 05:46:46,587 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.intermediate.dense.bias\n2021-07-27 05:46:46,588 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.output.dense.weight\n2021-07-27 05:46:46,589 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.output.dense.bias\n2021-07-27 05:46:46,590 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.output.LayerNorm.weight\n2021-07-27 05:46:46,591 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.23.output.LayerNorm.bias\n2021-07-27 05:46:46,593 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n2021-07-27 05:46:46,594 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n2021-07-27 05:46:46,595 - INFO - allennlp.common.util - excerpt_encoder.pooler.dense.weight\n2021-07-27 05:46:46,596 - INFO - allennlp.common.util - excerpt_encoder.pooler.dense.bias\n2021-07-27 05:46:46,597 - INFO - allennlp.common.util - hostname_embedder.token_embedder_tokens.weight\n2021-07-27 05:46:46,598 - INFO - allennlp.common.util - classification_layer.weight\n2021-07-27 05:46:46,600 - INFO - allennlp.common.util - classification_layer.bias\n2021-07-27 05:46:46,601 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n2021-07-27 05:46:46,603 - WARNING - allennlp.common.from_params - Parameter num_epochs for class SlantedTriangular was found in both **extras and in params. Using the specification found in params, but you probably put a key in a config file that you didn't need, and if it is different from what we get from **extras, you might get unexpected behavior.\n2021-07-27 05:46:46,604 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_epochs = 10\n2021-07-27 05:46:46,605 - WARNING - allennlp.common.from_params - Parameter num_steps_per_epoch for class SlantedTriangular was found in both **extras and in params. Using the specification found in params, but you probably put a key in a config file that you didn't need, and if it is different from what we get from **extras, you might get unexpected behavior.\n2021-07-27 05:46:46,606 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_steps_per_epoch = 3088\n2021-07-27 05:46:46,607 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06\n2021-07-27 05:46:46,608 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n2021-07-27 05:46:46,609 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n2021-07-27 05:46:46,611 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n2021-07-27 05:46:46,612 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n2021-07-27 05:46:46,613 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n2021-07-27 05:46:46,615 - INFO - allennlp.common.params - type = default\n2021-07-27 05:46:46,616 - INFO - allennlp.common.params - save_completed_epochs = True\n2021-07-27 05:46:46,617 - INFO - allennlp.common.params - save_every_num_seconds = None\n2021-07-27 05:46:46,618 - INFO - allennlp.common.params - save_every_num_batches = None\n2021-07-27 05:46:46,620 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n2021-07-27 05:46:46,621 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n2021-07-27 05:46:46,622 - WARNING - allennlp.training.gradient_descent_trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n2021-07-27 05:46:46,629 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n2021-07-27 05:46:46,631 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/14\n2021-07-27 05:46:46,632 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.7G\n2021-07-27 05:46:46,634 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 1.3G\n2021-07-27 05:46:46,641 - INFO - allennlp.training.gradient_descent_trainer - Training\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/284 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4995e0b0add24cde82248fcc10ca79f1"}},"metadata":{}},{"name":"stdout","text":"2021-07-27 05:46:48,756 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2021-07-27 05:46:48,757 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/token_ids (Shape: 8 x 245)\ntensor([[    0,  1779,  7393,  ...,     1,     1,     1],\n        [    0,   133,  3939,  ...,     1,     1,     1],\n        [    0,  1121,  4634,  ...,     1,     1,     1],\n        ...,\n        [    0,   100,    56,  ...,     1,     1,     1],\n        [    0, 46150, 23736,  ...,  1085,    72,     2],\n        [    0, 47003,  9013,  ...,     1,     1,     1]], device='cuda:0')\n2021-07-27 05:46:48,764 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/mask (Shape: 8 x 245)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True,  True,  True],\n        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n2021-07-27 05:46:48,771 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/type_ids (Shape: 8 x 245)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n2021-07-27 05:46:48,779 - INFO - allennlp.training.callbacks.console_logger - batch_input/hostname/tokens/tokens (Shape: 8 x 1)\ntensor([[2],\n        [2],\n        [2],\n        ...,\n        [2],\n        [2],\n        [5]], device='cuda:0')\n2021-07-27 05:46:48,783 - INFO - allennlp.training.callbacks.console_logger - batch_input/target (Shape: 8)\ntensor([ 1.1243, -0.8585, -0.4144,  ..., -1.6596, -0.8073, -2.5504],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"from allennlp.models.archival import load_archive\n\narchive = load_archive(\"serialization/1/model.tar.gz\")\npredictor = RegressorPredictor.from_archive(archive)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from urllib.parse import urlparse\n\n\ntest_df = pandas.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nprint(test_df.head())\n\ntest_df[\"hostname\"] = test_df \\\n    .url_legal \\\n    .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\nbatch_json = test_df.apply(lambda row: {\"excerpt\": row.excerpt, \"hostname\": row.hostname}, axis=1).tolist()\npredictor.predict_batch_json(batch_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BatchIterator:\n        def __init__(self, data, batch_size):\n                self.data = data\n                self.batch_size = batch_size\n                self.cur = 0\n            \n        def __iter__(self):\n                return self\n            \n        def __next__(self):\n                batch = self.data[self.cur:self.cur+self.batch_size]\n                self.cur += self.batch_size\n                if len(batch) == 0:\n                    raise StopIteration\n                return batch\n\n\npredictions = []\nbatch_iterator = BatchIterator(batch_json, batch_size=32)\n\nfor batch in batch_iterator:\n    predictions += predictor.predict_batch_json(batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"target\"] = list(map(lambda p: p[\"logit\"][0], predictions))\ntest_df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}