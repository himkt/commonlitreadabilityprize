{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T13:39:16.334356Z","iopub.execute_input":"2021-07-26T13:39:16.334770Z","iopub.status.idle":"2021-07-26T13:39:16.363854Z","shell.execute_reply.started":"2021-07-26T13:39:16.334682Z","shell.execute_reply":"2021-07-26T13:39:16.362997Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/glove-vec/glove.6B.100d.word2vec\n/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n/kaggle/input/commonlitreadabilityprize/train.csv\n/kaggle/input/commonlitreadabilityprize/test.csv\n/kaggle/input/roberta-base/config.json\n/kaggle/input/roberta-base/merges.txt\n/kaggle/input/roberta-base/vocab.json\n/kaggle/input/roberta-base/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas\nimport numpy\n\n\ndf = pandas.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nnum_records = len(df)\n\nids = numpy.arange(num_records)\nids = numpy.random.permutation(ids)\n\ntrain_size = 0.8\npartition = int(num_records * train_size)\n\ntrain_ids, valid_ids = ids[:partition], ids[partition:]\n\ndf.loc[train_ids].to_csv(\"./processed_train.csv\", index=False)\ndf.loc[valid_ids].to_csv(\"./processed_valid.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:39:16.366954Z","iopub.execute_input":"2021-07-26T13:39:16.367287Z","iopub.status.idle":"2021-07-26T13:39:16.690402Z","shell.execute_reply.started":"2021-07-26T13:39:16.367258Z","shell.execute_reply":"2021-07-26T13:39:16.689369Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from typing import Any, Dict, Iterable, MutableMapping, Optional\nfrom urllib.parse import urlparse\n\nfrom allennlp.data import DatasetReader\nfrom allennlp.data import Tokenizer\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields import ArrayField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer\nimport pandas\nimport numpy\nfrom overrides import overrides\n\n\n@DatasetReader.register(\"commonlit_reader\")\nclass CommonlitDatasetReader(DatasetReader):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n    ) -> None:\n\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.token_indexers: Dict[str, TokenIndexer] = token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        instances = []\n\n        dataframe = pandas.read_csv(file_path)\n        dataframe[\"hostname\"] = dataframe \\\n            .url_legal \\\n            .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\n        for _, row in dataframe.iterrows():\n            excerpt = row.excerpt\n            target = row.target if hasattr(row, \"target\") else None\n            instances.append(self.text_to_instance(excerpt, target))\n\n        return instances\n\n    @overrides\n    def text_to_instance(self, excerpt: str, target: Optional[float] = None) -> Instance:\n        tokens = self.tokenizer.tokenize(excerpt)\n        fields: MutableMapping[str, Field[Any]] = {\n            \"excerpt\": TextField(tokens),\n        }\n        if target is not None:\n            fields[\"target\"] = ArrayField(numpy.asarray(target, dtype=numpy.float32))\n        return Instance(fields=fields)\n\n    def apply_token_indexers(self, instance: Instance) -> None:\n        assert isinstance(instance.fields[\"excerpt\"], TextField)\n        instance.fields[\"excerpt\"].token_indexers = self.token_indexers\n\n        \nfrom typing import Dict, Optional\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules import TextFieldEmbedder\nfrom allennlp.modules import Seq2VecEncoder\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.data.fields.text_field import TextFieldTensors\nfrom overrides.overrides import overrides\nfrom torch import FloatTensor\nfrom torch.functional import Tensor\nfrom torch.nn.functional import mse_loss\nfrom torch import sqrt\nfrom torch.nn import Linear\n\n\nEPS = 1e-8\n\n\n@Model.register(\"baseline\")\nclass BaselineRegressor(Model):\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        excerpt_embedder: TextFieldEmbedder,\n        excerpt_encoder: Seq2VecEncoder,\n    ) -> None:\n\n        super().__init__(vocab)\n\n        self.vocab = vocab\n        self.excerpt_embedder = excerpt_embedder\n        self.excerpt_encoder = excerpt_encoder\n\n        self.dense = Linear(\n            in_features=self.excerpt_encoder.get_output_dim(),\n            out_features=1,\n        )\n\n    @overrides\n    def forward(\n        self,\n        excerpt: TextFieldTensors,\n        target: Optional[FloatTensor] = None,\n    ) -> Dict[str, Tensor]:\n\n        mask = get_text_field_mask(excerpt)\n        excerpt_emb = self.excerpt_embedder(excerpt)\n        logit = self.dense(self.excerpt_encoder(excerpt_emb, mask=mask))\n\n        output_dict = {\"logit\": logit}\n        if target is not None:\n            output_dict[\"loss\"] = sqrt(mse_loss(logit.view(-1), target) + EPS)\n\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {}\n\n    \nfrom allennlp.common.util import JsonDict\nfrom allennlp.data.instance import Instance\nfrom allennlp.predictors import Predictor\n\n\n@Predictor.register(\"regressor_predictor\")\nclass RegressorPredictor(Predictor):\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        return self._dataset_reader.text_to_instance(**json_dict)  # type: ignore","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:39:16.692919Z","iopub.execute_input":"2021-07-26T13:39:16.693291Z","iopub.status.idle":"2021-07-26T13:39:26.722028Z","shell.execute_reply.started":"2021-07-26T13:39:16.693252Z","shell.execute_reply":"2021-07-26T13:39:26.721150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!ls ../input","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:39:26.725501Z","iopub.execute_input":"2021-07-26T13:39:26.725783Z","iopub.status.idle":"2021-07-26T13:39:27.434500Z","shell.execute_reply.started":"2021-07-26T13:39:26.725755Z","shell.execute_reply":"2021-07-26T13:39:27.433488Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"commonlitreadabilityprize  glove-vec  roberta-base\n","output_type":"stream"}]},{"cell_type":"code","source":"jsonnet_text = \"\"\"\\\n{\n    dataset_reader: {\n        type: \"commonlit_reader\",\n        tokenizer: {\n            type: \"pretrained_transformer\",\n            model_name: \"../input/roberta-base\",\n        },\n        token_indexers: {\n            tokens: {\n                type: \"pretrained_transformer\",\n                model_name: \"../input/roberta-base\",\n            },\n        },\n    },\n    train_data_path: \"./processed_train.csv\",\n    validation_data_path: \"./processed_valid.csv\",\n    model: {\n        type: \"baseline\",\n        excerpt_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    type: \"pretrained_transformer\",\n                    model_name: \"../input/roberta-base\",\n                },\n            },\n        },\n        excerpt_encoder: {\n            type: \"bert_pooler\",\n            pretrained_model: \"../input/roberta-base\",\n        }\n    },\n    trainer: {\n        num_epochs: 30,\n        learning_rate_scheduler: {\n            type: \"slanted_triangular\",\n            num_epochs: 10,\n            num_steps_per_epoch: 3088,\n            cut_frac: 0.06\n        },\n        optimizer: {\n            type: \"huggingface_adamw\",\n            lr: 2e-5,\n            weight_decay: 0.1,\n        },\n        validation_metric: \"-loss\"\n    },\n    data_loader: {\n        batch_size: 16,\n        shuffle: true\n    }\n}\n\n\"\"\"\n\nf = open(\"baseline.jsonnet\", \"w\")\nf.write(jsonnet_text)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:39:27.438109Z","iopub.execute_input":"2021-07-26T13:39:27.438393Z","iopub.status.idle":"2021-07-26T13:39:27.446727Z","shell.execute_reply.started":"2021-07-26T13:39:27.438363Z","shell.execute_reply":"2021-07-26T13:39:27.445349Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import allennlp.commands\n\nallennlp.commands.train.train_model_from_file(\n    parameter_filename=\"./baseline.jsonnet\",\n    serialization_dir=\"./serialization/4\",\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:39:27.448263Z","iopub.execute_input":"2021-07-26T13:39:27.448677Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"2021-07-26 13:39:30,443 - INFO - allennlp.common.params - random_seed = 13370\n2021-07-26 13:39:30,444 - INFO - allennlp.common.params - numpy_seed = 1337\n2021-07-26 13:39:30,445 - INFO - allennlp.common.params - pytorch_seed = 133\n2021-07-26 13:39:30,495 - INFO - allennlp.common.checks - Pytorch version: 1.7.0\n2021-07-26 13:39:30,497 - INFO - allennlp.common.params - type = default\n2021-07-26 13:39:30,499 - INFO - allennlp.common.params - dataset_reader.type = commonlit_reader\n2021-07-26 13:39:30,501 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n2021-07-26 13:39:30,502 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = ../input/roberta-base\n2021-07-26 13:39:30,504 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n2021-07-26 13:39:30,505 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None\n2021-07-26 13:39:30,506 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n2021-07-26 13:39:31,255 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer\n2021-07-26 13:39:31,256 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n2021-07-26 13:39:31,257 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = ../input/roberta-base\n2021-07-26 13:39:31,259 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n2021-07-26 13:39:31,261 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = None\n2021-07-26 13:39:31,263 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n2021-07-26 13:39:31,266 - INFO - allennlp.common.params - train_data_path = ./processed_train.csv\n2021-07-26 13:39:31,268 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f33efd87210>\n2021-07-26 13:39:31,269 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n2021-07-26 13:39:31,270 - INFO - allennlp.common.params - validation_dataset_reader = None\n2021-07-26 13:39:31,276 - INFO - allennlp.common.params - validation_data_path = ./processed_valid.csv\n2021-07-26 13:39:31,277 - INFO - allennlp.common.params - validation_data_loader = None\n2021-07-26 13:39:31,278 - INFO - allennlp.common.params - test_data_path = None\n2021-07-26 13:39:31,279 - INFO - allennlp.common.params - evaluate_on_test = False\n2021-07-26 13:39:31,280 - INFO - allennlp.common.params - batch_weight_key = \n2021-07-26 13:39:31,281 - INFO - allennlp.common.params - data_loader.type = multiprocess\n2021-07-26 13:39:31,283 - INFO - allennlp.common.params - data_loader.batch_size = 16\n2021-07-26 13:39:31,288 - INFO - allennlp.common.params - data_loader.drop_last = False\n2021-07-26 13:39:31,289 - INFO - allennlp.common.params - data_loader.shuffle = True\n2021-07-26 13:39:31,290 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n2021-07-26 13:39:31,291 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n2021-07-26 13:39:31,292 - INFO - allennlp.common.params - data_loader.num_workers = 0\n2021-07-26 13:39:31,293 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n2021-07-26 13:39:31,294 - INFO - allennlp.common.params - data_loader.start_method = fork\n2021-07-26 13:39:31,295 - INFO - allennlp.common.params - data_loader.cuda_device = None\n2021-07-26 13:39:31,296 - INFO - allennlp.common.params - data_loader.quiet = False\n2021-07-26 13:39:31,297 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f3443bf4e10>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"loading instances: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a559e914456745b4a6876b0fb50a0e15"}},"metadata":{}},{"name":"stdout","text":"2021-07-26 13:39:36,497 - INFO - allennlp.common.params - data_loader.type = multiprocess\n2021-07-26 13:39:36,499 - INFO - allennlp.common.params - data_loader.batch_size = 16\n2021-07-26 13:39:36,501 - INFO - allennlp.common.params - data_loader.drop_last = False\n2021-07-26 13:39:36,502 - INFO - allennlp.common.params - data_loader.shuffle = True\n2021-07-26 13:39:36,503 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n2021-07-26 13:39:36,504 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n2021-07-26 13:39:36,505 - INFO - allennlp.common.params - data_loader.num_workers = 0\n2021-07-26 13:39:36,506 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n2021-07-26 13:39:36,507 - INFO - allennlp.common.params - data_loader.start_method = fork\n2021-07-26 13:39:36,508 - INFO - allennlp.common.params - data_loader.cuda_device = None\n2021-07-26 13:39:36,509 - INFO - allennlp.common.params - data_loader.quiet = False\n2021-07-26 13:39:36,510 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f3443bf4e10>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"loading instances: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abc8a0872c646f1bf7ea87ea6487102"}},"metadata":{}},{"name":"stdout","text":"2021-07-26 13:39:37,692 - INFO - allennlp.common.params - type = from_instances\n2021-07-26 13:39:37,693 - INFO - allennlp.common.params - min_count = None\n2021-07-26 13:39:37,694 - INFO - allennlp.common.params - max_vocab_size = None\n2021-07-26 13:39:37,695 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n2021-07-26 13:39:37,696 - INFO - allennlp.common.params - pretrained_files = None\n2021-07-26 13:39:37,697 - INFO - allennlp.common.params - only_include_pretrained_words = False\n2021-07-26 13:39:37,697 - INFO - allennlp.common.params - tokens_to_add = None\n2021-07-26 13:39:37,698 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n2021-07-26 13:39:37,699 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n2021-07-26 13:39:37,705 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n2021-07-26 13:39:37,711 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"building vocab: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee30a0220d84f4aa8104533daf16343"}},"metadata":{}},{"name":"stdout","text":"2021-07-26 13:39:37,862 - INFO - allennlp.common.params - model.type = baseline\n2021-07-26 13:39:37,863 - INFO - allennlp.common.params - model.excerpt_embedder.type = basic\n2021-07-26 13:39:37,865 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.type = pretrained_transformer\n2021-07-26 13:39:37,866 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.model_name = ../input/roberta-base\n2021-07-26 13:39:37,867 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.max_length = None\n2021-07-26 13:39:37,868 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.sub_module = None\n2021-07-26 13:39:37,869 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.train_parameters = True\n2021-07-26 13:39:37,869 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.eval_mode = False\n2021-07-26 13:39:37,871 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.last_layer_only = True\n2021-07-26 13:39:37,877 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.override_weights_file = None\n2021-07-26 13:39:37,878 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n2021-07-26 13:39:37,879 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.load_weights = True\n2021-07-26 13:39:37,879 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.gradient_checkpointing = None\n2021-07-26 13:39:37,881 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.tokenizer_kwargs = None\n2021-07-26 13:39:37,882 - INFO - allennlp.common.params - model.excerpt_embedder.token_embedders.tokens.transformer_kwargs = None\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"2021-07-26 13:39:45,467 - INFO - allennlp.common.params - model.excerpt_encoder.type = bert_pooler\n2021-07-26 13:39:45,468 - INFO - allennlp.common.params - model.excerpt_encoder.pretrained_model = ../input/roberta-base\n2021-07-26 13:39:45,470 - INFO - allennlp.common.params - model.excerpt_encoder.override_weights_file = None\n2021-07-26 13:39:45,473 - INFO - allennlp.common.params - model.excerpt_encoder.override_weights_strip_prefix = None\n2021-07-26 13:39:45,473 - INFO - allennlp.common.params - model.excerpt_encoder.load_weights = True\n2021-07-26 13:39:45,474 - INFO - allennlp.common.params - model.excerpt_encoder.requires_grad = True\n2021-07-26 13:39:45,478 - INFO - allennlp.common.params - model.excerpt_encoder.dropout = 0.0\n2021-07-26 13:39:45,478 - INFO - allennlp.common.params - model.excerpt_encoder.transformer_kwargs = None\n2021-07-26 13:39:45,482 - INFO - filelock - Lock 139860806523664 acquired on ./serialization/4/vocabulary/.lock\n2021-07-26 13:39:45,484 - INFO - filelock - Lock 139860806523664 released on ./serialization/4/vocabulary/.lock\n2021-07-26 13:39:45,764 - INFO - allennlp.common.params - trainer.type = gradient_descent\n2021-07-26 13:39:45,766 - INFO - allennlp.common.params - trainer.cuda_device = None\n2021-07-26 13:39:45,766 - INFO - allennlp.common.params - trainer.distributed = False\n2021-07-26 13:39:45,767 - INFO - allennlp.common.params - trainer.world_size = 1\n2021-07-26 13:39:45,768 - INFO - allennlp.common.params - trainer.patience = None\n2021-07-26 13:39:45,769 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n2021-07-26 13:39:45,775 - INFO - allennlp.common.params - trainer.num_epochs = 30\n2021-07-26 13:39:45,776 - INFO - allennlp.common.params - trainer.grad_norm = None\n2021-07-26 13:39:45,777 - INFO - allennlp.common.params - trainer.grad_clipping = None\n2021-07-26 13:39:45,778 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n2021-07-26 13:39:45,778 - INFO - allennlp.common.params - trainer.use_amp = False\n2021-07-26 13:39:45,779 - INFO - allennlp.common.params - trainer.no_grad = None\n2021-07-26 13:39:45,781 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n2021-07-26 13:39:45,781 - INFO - allennlp.common.params - trainer.moving_average = None\n2021-07-26 13:39:45,782 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f33f11b3110>\n2021-07-26 13:39:45,789 - INFO - allennlp.common.params - trainer.callbacks = None\n2021-07-26 13:39:45,790 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n2021-07-26 13:39:45,791 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n2021-07-26 13:39:52,135 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n2021-07-26 13:39:52,139 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n2021-07-26 13:39:52,142 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05\n2021-07-26 13:39:52,145 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n2021-07-26 13:39:52,148 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n2021-07-26 13:39:52,151 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n2021-07-26 13:39:52,152 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n2021-07-26 13:39:52,154 - INFO - allennlp.training.optimizers - Number of trainable parameters: 125236993\n2021-07-26 13:39:52,160 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n2021-07-26 13:39:52,169 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n2021-07-26 13:39:52,172 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n2021-07-26 13:39:52,173 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n2021-07-26 13:39:52,175 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n2021-07-26 13:39:52,176 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n2021-07-26 13:39:52,179 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n2021-07-26 13:39:52,180 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n2021-07-26 13:39:52,181 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n2021-07-26 13:39:52,182 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n2021-07-26 13:39:52,183 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n2021-07-26 13:39:52,189 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n2021-07-26 13:39:52,190 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n2021-07-26 13:39:52,191 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n2021-07-26 13:39:52,191 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n2021-07-26 13:39:52,192 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,193 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,194 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n2021-07-26 13:39:52,195 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n2021-07-26 13:39:52,199 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n2021-07-26 13:39:52,200 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n2021-07-26 13:39:52,201 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n2021-07-26 13:39:52,202 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n2021-07-26 13:39:52,204 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n2021-07-26 13:39:52,207 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n2021-07-26 13:39:52,208 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n2021-07-26 13:39:52,209 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n2021-07-26 13:39:52,211 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n2021-07-26 13:39:52,212 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n2021-07-26 13:39:52,215 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n2021-07-26 13:39:52,216 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n2021-07-26 13:39:52,217 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,217 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,222 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n2021-07-26 13:39:52,223 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n2021-07-26 13:39:52,224 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n2021-07-26 13:39:52,224 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n2021-07-26 13:39:52,225 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n2021-07-26 13:39:52,226 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n2021-07-26 13:39:52,229 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n2021-07-26 13:39:52,229 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n2021-07-26 13:39:52,230 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n2021-07-26 13:39:52,232 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n2021-07-26 13:39:52,235 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n2021-07-26 13:39:52,236 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n2021-07-26 13:39:52,236 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n2021-07-26 13:39:52,237 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n2021-07-26 13:39:52,240 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,241 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,242 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n2021-07-26 13:39:52,244 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n2021-07-26 13:39:52,247 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n2021-07-26 13:39:52,248 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n2021-07-26 13:39:52,248 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n2021-07-26 13:39:52,249 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n2021-07-26 13:39:52,251 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n2021-07-26 13:39:52,253 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n2021-07-26 13:39:52,254 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n2021-07-26 13:39:52,255 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n2021-07-26 13:39:52,257 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n2021-07-26 13:39:52,258 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n2021-07-26 13:39:52,260 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n2021-07-26 13:39:52,261 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n2021-07-26 13:39:52,263 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,263 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,264 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n2021-07-26 13:39:52,267 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n2021-07-26 13:39:52,268 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n2021-07-26 13:39:52,269 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n2021-07-26 13:39:52,270 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n2021-07-26 13:39:52,273 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n2021-07-26 13:39:52,274 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n2021-07-26 13:39:52,275 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n2021-07-26 13:39:52,276 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n2021-07-26 13:39:52,277 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n2021-07-26 13:39:52,283 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n2021-07-26 13:39:52,284 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n2021-07-26 13:39:52,284 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n2021-07-26 13:39:52,285 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n2021-07-26 13:39:52,286 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,287 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,288 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n2021-07-26 13:39:52,289 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n2021-07-26 13:39:52,290 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n2021-07-26 13:39:52,293 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n2021-07-26 13:39:52,294 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n2021-07-26 13:39:52,294 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n2021-07-26 13:39:52,295 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n2021-07-26 13:39:52,298 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n2021-07-26 13:39:52,299 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n2021-07-26 13:39:52,307 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n2021-07-26 13:39:52,308 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n2021-07-26 13:39:52,309 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n2021-07-26 13:39:52,310 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n2021-07-26 13:39:52,317 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n2021-07-26 13:39:52,318 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,319 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,328 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n2021-07-26 13:39:52,329 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n2021-07-26 13:39:52,329 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n2021-07-26 13:39:52,330 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n2021-07-26 13:39:52,331 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n2021-07-26 13:39:52,332 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n2021-07-26 13:39:52,341 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n2021-07-26 13:39:52,342 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n2021-07-26 13:39:52,343 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n2021-07-26 13:39:52,344 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n2021-07-26 13:39:52,354 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n2021-07-26 13:39:52,355 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n2021-07-26 13:39:52,356 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n2021-07-26 13:39:52,357 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n2021-07-26 13:39:52,357 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,358 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,365 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n2021-07-26 13:39:52,366 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n2021-07-26 13:39:52,367 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n2021-07-26 13:39:52,368 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n2021-07-26 13:39:52,369 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n2021-07-26 13:39:52,369 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n2021-07-26 13:39:52,370 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n2021-07-26 13:39:52,371 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n2021-07-26 13:39:52,372 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n2021-07-26 13:39:52,379 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n2021-07-26 13:39:52,380 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n2021-07-26 13:39:52,381 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n2021-07-26 13:39:52,382 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n2021-07-26 13:39:52,383 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n2021-07-26 13:39:52,384 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,385 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,386 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n2021-07-26 13:39:52,387 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n2021-07-26 13:39:52,388 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n2021-07-26 13:39:52,389 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n2021-07-26 13:39:52,396 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n2021-07-26 13:39:52,398 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n2021-07-26 13:39:52,398 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n2021-07-26 13:39:52,399 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n2021-07-26 13:39:52,400 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n2021-07-26 13:39:52,401 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n2021-07-26 13:39:52,402 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n2021-07-26 13:39:52,403 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n2021-07-26 13:39:52,404 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n2021-07-26 13:39:52,405 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n2021-07-26 13:39:52,406 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,408 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,409 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n2021-07-26 13:39:52,410 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n2021-07-26 13:39:52,411 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n2021-07-26 13:39:52,412 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n2021-07-26 13:39:52,413 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n2021-07-26 13:39:52,414 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n2021-07-26 13:39:52,416 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n2021-07-26 13:39:52,417 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n2021-07-26 13:39:52,418 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n2021-07-26 13:39:52,419 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n2021-07-26 13:39:52,427 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n2021-07-26 13:39:52,428 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n2021-07-26 13:39:52,429 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n2021-07-26 13:39:52,430 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n2021-07-26 13:39:52,433 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,433 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,434 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n2021-07-26 13:39:52,436 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n2021-07-26 13:39:52,440 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n2021-07-26 13:39:52,440 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n2021-07-26 13:39:52,441 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n2021-07-26 13:39:52,442 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n2021-07-26 13:39:52,445 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n2021-07-26 13:39:52,446 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n2021-07-26 13:39:52,446 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n2021-07-26 13:39:52,448 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n2021-07-26 13:39:52,450 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n2021-07-26 13:39:52,451 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n2021-07-26 13:39:52,452 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n2021-07-26 13:39:52,452 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n2021-07-26 13:39:52,453 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,454 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,455 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n2021-07-26 13:39:52,456 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n2021-07-26 13:39:52,457 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n2021-07-26 13:39:52,457 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n2021-07-26 13:39:52,458 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n2021-07-26 13:39:52,459 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n2021-07-26 13:39:52,460 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n2021-07-26 13:39:52,461 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n2021-07-26 13:39:52,461 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n2021-07-26 13:39:52,462 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n2021-07-26 13:39:52,464 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n2021-07-26 13:39:52,465 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n2021-07-26 13:39:52,466 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n2021-07-26 13:39:52,467 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n2021-07-26 13:39:52,468 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n2021-07-26 13:39:52,468 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n2021-07-26 13:39:52,469 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n2021-07-26 13:39:52,470 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n2021-07-26 13:39:52,471 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n2021-07-26 13:39:52,472 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n2021-07-26 13:39:52,473 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n2021-07-26 13:39:52,474 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n2021-07-26 13:39:52,475 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n2021-07-26 13:39:52,476 - INFO - allennlp.common.util - excerpt_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n2021-07-26 13:39:52,477 - INFO - allennlp.common.util - excerpt_encoder.pooler.dense.weight\n2021-07-26 13:39:52,478 - INFO - allennlp.common.util - excerpt_encoder.pooler.dense.bias\n2021-07-26 13:39:52,479 - INFO - allennlp.common.util - dense.weight\n2021-07-26 13:39:52,480 - INFO - allennlp.common.util - dense.bias\n2021-07-26 13:39:52,482 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n2021-07-26 13:39:52,484 - WARNING - allennlp.common.from_params - Parameter num_epochs for class SlantedTriangular was found in both **extras and in params. Using the specification found in params, but you probably put a key in a config file that you didn't need, and if it is different from what we get from **extras, you might get unexpected behavior.\n2021-07-26 13:39:52,485 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_epochs = 10\n2021-07-26 13:39:52,486 - WARNING - allennlp.common.from_params - Parameter num_steps_per_epoch for class SlantedTriangular was found in both **extras and in params. Using the specification found in params, but you probably put a key in a config file that you didn't need, and if it is different from what we get from **extras, you might get unexpected behavior.\n2021-07-26 13:39:52,486 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_steps_per_epoch = 3088\n2021-07-26 13:39:52,487 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06\n2021-07-26 13:39:52,490 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n2021-07-26 13:39:52,491 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n2021-07-26 13:39:52,492 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n2021-07-26 13:39:52,492 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n2021-07-26 13:39:52,493 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n2021-07-26 13:39:52,494 - INFO - allennlp.common.params - type = default\n2021-07-26 13:39:52,495 - INFO - allennlp.common.params - save_completed_epochs = True\n2021-07-26 13:39:52,496 - INFO - allennlp.common.params - save_every_num_seconds = None\n2021-07-26 13:39:52,497 - INFO - allennlp.common.params - save_every_num_batches = None\n2021-07-26 13:39:52,498 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n2021-07-26 13:39:52,499 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n2021-07-26 13:39:52,500 - WARNING - allennlp.training.gradient_descent_trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n2021-07-26 13:39:52,504 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n2021-07-26 13:39:52,505 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/29\n2021-07-26 13:39:52,506 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 4.0G\n2021-07-26 13:39:52,507 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 479M\n2021-07-26 13:39:52,509 - INFO - allennlp.training.gradient_descent_trainer - Training\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/142 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f37de751494c73ae07335dbd3f42ab"}},"metadata":{}},{"name":"stdout","text":"2021-07-26 13:39:53,922 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2021-07-26 13:39:53,923 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/token_ids (Shape: 16 x 248)\ntensor([[    0,  2409,     5,  ...,     1,     1,     1],\n        [    0,  3762,   183,  ...,     1,     1,     1],\n        [    0,  4148,     5,  ...,     1,     1,     1],\n        ...,\n        [    0,  9962,  1141,  ...,     1,     1,     1],\n        [    0, 32136,   155,  ...,     1,     1,     1],\n        [    0,  3750,   204,  ...,     1,     1,     1]], device='cuda:0')\n2021-07-26 13:39:53,930 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/mask (Shape: 16 x 248)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n2021-07-26 13:39:53,935 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/type_ids (Shape: 16 x 248)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n2021-07-26 13:39:53,940 - INFO - allennlp.training.callbacks.console_logger - batch_input/target (Shape: 16)\ntensor([-1.6790, -0.1703, -2.4660,  ..., -1.1560,  0.2818, -0.9677],\n       device='cuda:0')\n2021-07-26 13:40:57,925 - INFO - allennlp.training.gradient_descent_trainer - Validating\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cba36cf000949ecaa2a5edf3bb4e196"}},"metadata":{}},{"name":"stdout","text":"2021-07-26 13:40:58,111 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n2021-07-26 13:40:58,112 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/token_ids (Shape: 16 x 258)\ntensor([[    0,   133,  3939,  ...,     1,     1,     1],\n        [    0,  4688, 40701,  ...,     1,     1,     1],\n        [    0, 13624,    52,  ...,     2,     1,     1],\n        ...,\n        [    0, 38056,    74,  ...,     1,     1,     1],\n        [    0,   133,  1385,  ...,     1,     1,     1],\n        [    0,   243,    21,  ...,     1,     1,     1]], device='cuda:0')\n2021-07-26 13:40:58,118 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/mask (Shape: 16 x 258)\ntensor([[ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ...,  True, False, False],\n        ...,\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False],\n        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')\n2021-07-26 13:40:58,124 - INFO - allennlp.training.callbacks.console_logger - batch_input/excerpt/tokens/type_ids (Shape: 16 x 258)\ntensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n2021-07-26 13:40:58,129 - INFO - allennlp.training.callbacks.console_logger - batch_input/target (Shape: 16)\ntensor([-0.5036, -0.2042, -0.1160,  ..., -1.4359, -0.8014, -0.4081],\n       device='cuda:0')\n2021-07-26 13:41:03,112 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n2021-07-26 13:41:03,113 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |   478.979  |       N/A\n2021-07-26 13:41:03,114 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.099  |     0.817\n2021-07-26 13:41:03,115 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  4069.371  |       N/A\n2021-07-26 13:41:09,198 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:01:16.692982\n2021-07-26 13:41:09,199 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:34:07\n2021-07-26 13:41:09,200 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/29\n2021-07-26 13:41:09,202 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 4.3G\n2021-07-26 13:41:09,204 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 7.9G\n2021-07-26 13:41:09,208 - INFO - allennlp.training.gradient_descent_trainer - Training\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/142 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3545bf247f8949d593c669b63f7a0de9"}},"metadata":{}}]},{"cell_type":"code","source":"from allennlp.models.archival import load_archive\n\narchive = load_archive(\"serialization/2/model.tar.gz\")\npredictor = RegressorPredictor.from_archive(archive)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pandas.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nprint(test_df.head())\n\nbatch_json = test_df.excerpt.apply(lambda excerpt: {\"excerpt\": excerpt}).tolist()\npredictor.predict_batch_json(batch_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BatchIterator:\n        def __init__(self, data, batch_size):\n                self.data = data\n                self.batch_size = batch_size\n                self.cur = 0\n            \n        def __iter__(self):\n                return self\n            \n        def __next__(self):\n                batch = self.data[self.cur:self.cur+self.batch_size]\n                self.cur += self.batch_size\n                if len(batch) == 0:\n                    raise StopIteration\n                return batch\n\n\npredictions = []\nbatch_iterator = BatchIterator(batch_json, batch_size=32)\n\nfor batch in batch_iterator:\n    predictions += predictor.predict_batch_json(batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"target\"] = list(map(lambda p: p[\"logit\"][0], predictions))\ntest_df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}